{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Licensed under the Apache License, Version 2.0 (the \"License\"); you may\n",
    "# not use this file except in compliance with the License. You may obtain\n",
    "# a copy of the License at\n",
    "#\n",
    "#      http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n",
    "# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n",
    "# License for the specific language governing permissions and limitations\n",
    "# under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning in Tensorflow with Inception V3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Transfer learning is the process of taking a pre-trained model (the weights and parameters of a network that has been trained on a large dataset by somebody else) and “fine-tuning” the model with your own dataset. The idea is that this pre-trained model will act as a feature extractor. You will remove the last layer of the network and replace it with your own classifier (depending on what your problem space is). You then freeze the weights of all the other layers and train the network normally (Freezing the layers means not changing the weights during gradient descent/optimization).\n",
    "\n",
    "For this experiment we used Google's Inception-V3 pretrained model for Image Classification. This model consists of two parts:\n",
    "    - Feature extraction part with a convolutional neural network.\n",
    "    - Classification part with fully-connected and softmax layers.\n",
    "The pre-trained Inception-v3 model achieves state-of-the-art accuracy for recognizing general objects with 1000 classes. The model extracts general features from input images in the first part and classifies them based on those features in the second part.\n",
    "\n",
    "We will use this pre-trained model and re-train it it to classify aerial views of houses which have a swimming pool in their backyard. In order to do so we will build a new image dataset with aerial views coming from Google Images. \n",
    "\n",
    "The following chart shows how the data flows in the Inception v3 model, which is a Convolutional Neural Network with many layers and a complicated structure. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../doc/source/images/inception_flowchart.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In transfer learning, when you build a new model to classify your original dataset, you reuse the feature extraction part and re-train the classification part with your dataset. Since you don't have to train the feature extraction part (which is the most complex part of the model), you can train the model with less computational resources and training time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../doc/source/images/inception_transfer_learning.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to train the classification part of the model using the preprocessed data. The previous diagram shows the relationship between the preprocessing and the training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "For this experiment, we built two small image datasets (~500 images) from the Google Images Web site -- one with aerial views of houses without swimming pools and another one with aerial views of houses with swimming pools.\n",
    "\n",
    "After downloading the images, we took an extra step to visualize the images and remove the false positives. All the images are then saved in two different directories identifying the proper classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For example it resizes this image:\n",
    "\n",
    "<img src=\"../data/images/house_with_pool/house-429353_960_720.jpg\">\n",
    "\n",
    "#### To this 299 x 299 image: \n",
    "\n",
    "<img src=\"../data/images_resized/house_with_pool/house-429353_960_720.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our datasets ready we can move over to actually code our image classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize dataset images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Houses\n",
    "Range of images are between **0001** and **0511**. As some 'false positive' have been removed as well as 'garbage' images, not all the range is covered. For example, 0024, 0217, 0432 exist but not 0047 or 0410 or ...so try different image numbers.<br><br>\n",
    "To visualize a different image, double click on the displayed image below, the command will show up. Change the image number to display another one.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../data/images/house_without_pool/giethoorn-2368494__340.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Houses + Pools\n",
    "Range of images are between **0001** and **0504**. As some 'false positive' have been removed as well as 'garbage' images, not all the range is covered. For example, 0113, 0285, 0467 exist but not 0105 or 0302 or ...so try different image numbers.<br><br>\n",
    "To visualize a different image, double click on the displayed image below, the command will show up. Change the image number to display another one.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../data/images/house_with_pool/pool-238012__340.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports\n",
    "We put all the imports at the top, because this is what most Python developers would expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "from datetime import datetime\n",
    "import os.path\n",
    "import shutil\n",
    "import sys\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import graph_util\n",
    "from tensorflow.python.platform import gfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import helper function definitions\n",
    "We put a lot of the code in functions in a Python module. We could put them right in the notebook, but this should make the simplified notebook easier to read and make the functions easier to unit test. We don't want to hide the code though. If you are taking a deeper look, be sure to look into helper.py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import the helper functions from transferlearning/helper.py\n",
    "module_path = os.path.abspath('..')\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "from transferlearning import helper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters\n",
    "\n",
    "The following includes parameters that are tied to the particular model architecture\n",
    "we're using for Inception v3. These include things like tensor names and their\n",
    "sizes. If you want to adapt this script to work with another model, you will\n",
    "need to update these to reflect the values in the network you're using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Raw input images come from this dir in the git repo (or you can customize this to point to a new dir).\n",
    "# Only JPEG images are used.\n",
    "image_dir = '../data/images'\n",
    "\n",
    "# If stored_images_resized, images that have already been resized are used again w/o re-resizing\n",
    "stored_images_resized = '../data/images_resized'  # set to None to ignore\n",
    "\n",
    "# If stored_bottlenecks, supplement the image_dir collection with additional bottlenecks from this dir\n",
    "stored_bottlenecks = '../data/bottlenecks'  # set to None to ignore\n",
    "\n",
    "# Working files are in /tmp by default.\n",
    "tmp_dir = '/tmp'\n",
    "bottleneck_dir = os.path.join(tmp_dir, 'bottlenecks')\n",
    "images_resized_dir = os.path.join(tmp_dir, 'images_resized')\n",
    "summaries_dir = os.path.join(tmp_dir, 'retrain_logs')\n",
    "\n",
    "# Download the original inception model to/from here.\n",
    "model_dir = os.path.join(tmp_dir, 'inception')\n",
    "inception_url = 'http://download.tensorflow.org/models/image/imagenet/inception-2015-12-05.tgz'\n",
    "\n",
    "output_graph_orig = \"output_graph_orig.pb\"\n",
    "output_graph = \"output_graph.pb\"\n",
    "output_labels = \"output_labels.txt\"\n",
    "\n",
    "how_many_training_steps = 500\n",
    "learning_rate = 0.01\n",
    "testing_percentage = 10\n",
    "validation_percentage = 10\n",
    "eval_step_interval = 10\n",
    "train_batch_size = 100\n",
    "test_batch_size = -1\n",
    "validation_batch_size = 100\n",
    "print_misclassified_test_images = False\n",
    "\n",
    "final_tensor_name = \"final_result\"\n",
    "\n",
    "flip_left_right = False\n",
    "random_crop = 0\n",
    "random_scale = 0\n",
    "random_brightness = 0\n",
    "\n",
    "force_inception_download = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download the Inception model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if force_inception_download and os.path.isdir(model_dir):    \n",
    "    shutil.rmtree(model_dir)\n",
    "helper.maybe_download_and_extract(inception_url, model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the images\n",
    "\n",
    "The Inception model requires 299 X 299 pixel sizes.\n",
    "First copy the files from `stored_images_resized` into `images_resized_dir`.\n",
    "With these stored images that are already resized, we don't need to repeat the process.\n",
    "Next resize and copy the raw images from `image_dir` into `images_resized_dir`.\n",
    "\n",
    "Install: **pip install python-resize-image**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Use a fresh working dir for the resized images\n",
    "if os.path.isdir(images_resized_dir):\n",
    "    shutil.rmtree(images_resized_dir)\n",
    "os.mkdir(images_resized_dir)\n",
    "    \n",
    "subdirs = ('house_with_pool', 'house_without_pool')\n",
    "\n",
    "# Copy in the image files\n",
    "for subdir in subdirs:\n",
    "    dest_dir = os.path.join(images_resized_dir, subdir)\n",
    "    if not os.path.isdir(dest_dir):\n",
    "        os.mkdir(dest_dir)\n",
    "\n",
    "    # Copy the already resized files first, if any, from the repo or a custom dir\n",
    "    if stored_images_resized:\n",
    "        source_dir = os.path.join(stored_images_resized, subdir)\n",
    "        if os.path.isdir(source_dir):\n",
    "            for f in os.listdir(source_dir):\n",
    "                path = os.path.join(source_dir, f)\n",
    "                if (os.path.isfile(path)):\n",
    "                    shutil.copy(path, dest_dir)\n",
    "                    \n",
    "    # Copy/resize the remaining raw images into the images_resized_dir(s)\n",
    "    helper.resize_images(os.path.join(image_dir, subdir), dest_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Copy stored bottleneck files\n",
    "Many previously calculated bottleneck files are stored in `stored_bottlenecks`\n",
    "to improve our dataset size and reduce processing time. Copy them into the working `bottleneck_dir`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use a fresh working dir for the bottleneck files  \n",
    "if os.path.isdir(bottleneck_dir):    \n",
    "    shutil.rmtree(bottleneck_dir)\n",
    "os.mkdir(bottleneck_dir)\n",
    "\n",
    "subdirs = ('house_with_pool', 'house_without_pool')\n",
    "\n",
    "# Copy in the stored bottleneck files\n",
    "for subdir in subdirs:\n",
    "    dest_dir = os.path.join(bottleneck_dir, subdir)\n",
    "    if not os.path.isdir(dest_dir):\n",
    "        os.mkdir(dest_dir)\n",
    "\n",
    "    if stored_bottlenecks:\n",
    "        source_dir = os.path.join(stored_bottlenecks, subdir)\n",
    "        if os.path.isdir(source_dir):\n",
    "            for f in os.listdir(source_dir):\n",
    "                path = os.path.join(source_dir, f)\n",
    "                if (os.path.isfile(path)):\n",
    "                    shutil.copy(path, dest_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retraining\n",
    "\n",
    "The following scripts demonstrate how to take an Inception v3 architecture model trained on\n",
    "ImageNet images, and train a new top layer that can recognize other classes of\n",
    "images.\n",
    "\n",
    "The top layer receives as input a 2048-dimensional vector for each image. We\n",
    "train a softmax layer on top of this representation. Assuming the softmax layer\n",
    "contains N labels, this corresponds to learning N + 2048*N model parameters\n",
    "corresponding to the learned biases and weights.\n",
    "\n",
    "We have a folder called **House-Pool** and two subfolders called **Pool** and **House** containing each one a different set of images (with and without pools).<br> \n",
    "The subfolder names are important, since they define what label is applied to each image, but the filenames themselves don't matter. The label for each image is taken from the name of the subfolder it's in. This produces a new model file that can be loaded and run by any TensorFlow program, for example the label_image sample code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# do some cleanup\n",
    "if os.path.isfile(output_graph):\n",
    "    os.remove(output_graph)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Setup the directory we'll write summaries to for TensorBoard\n",
    "if tf.gfile.Exists(summaries_dir):\n",
    "    tf.gfile.DeleteRecursively(summaries_dir)\n",
    "tf.gfile.MakeDirs(summaries_dir)\n",
    "\n",
    "# Set up the pre-trained graph.\n",
    "graph, bottleneck_tensor, jpeg_data_tensor, resized_image_tensor = (\n",
    "      helper.create_inception_graph(model_dir))\n",
    "\n",
    "# Look at the folder structure, and create lists of all the images.\n",
    "image_lists = helper.create_image_lists(images_resized_dir, bottleneck_dir, testing_percentage,\n",
    "                                        validation_percentage)\n",
    "class_count = len(image_lists.keys()) if image_lists else 0\n",
    "if class_count == 0:\n",
    "    print('No valid folders of images found at ' + images_resized_dir)\n",
    "if class_count == 1:\n",
    "    print('Only one valid folder of images found at ' + images_resized_dir +\n",
    "          ' - multiple classes are needed for classification.')\n",
    "# See if the command-line flags mean we're applying any distortions.\n",
    "do_distort_images = helper.should_distort_images(\n",
    "      flip_left_right, random_crop, random_scale,\n",
    "      random_brightness)\n",
    "sess = tf.Session()\n",
    "\n",
    "if do_distort_images:\n",
    "    # We will be applying distortions, so setup the operations we'll need.\n",
    "    distorted_jpeg_data_tensor, distorted_image_tensor = helper.add_input_distortions(\n",
    "        flip_left_right, random_crop, random_scale,\n",
    "        random_brightness)\n",
    "else:\n",
    "    # We'll make sure we've calculated the 'bottleneck' image summaries and\n",
    "    # cached them on disk.\n",
    "    helper.cache_bottlenecks(sess, image_lists, images_resized_dir, bottleneck_dir,\n",
    "                             jpeg_data_tensor, bottleneck_tensor)\n",
    "\n",
    "# Add the new layer that we'll be training.\n",
    "(train_step, cross_entropy, bottleneck_input, ground_truth_input,\n",
    "   final_tensor) = helper.add_final_training_ops(len(image_lists.keys()),\n",
    "                                                 final_tensor_name,\n",
    "                                                 bottleneck_tensor)\n",
    "\n",
    "# Create the operations we need to evaluate the accuracy of our new layer.\n",
    "evaluation_step, prediction = helper.add_evaluation_step(\n",
    "      final_tensor, ground_truth_input)\n",
    "\n",
    "# Merge all the summaries and write them out to /tmp/retrain_logs (by default)\n",
    "merged = tf.summary.merge_all()\n",
    "train_writer = tf.summary.FileWriter(summaries_dir + '/train',\n",
    "                                       sess.graph)\n",
    "validation_writer = tf.summary.FileWriter(summaries_dir + '/validation')\n",
    "\n",
    "# Set up all our weights to their initial default values.\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "output_graph_def = graph_util.convert_variables_to_constants(\n",
    "    sess, graph.as_graph_def(), [final_tensor_name])\n",
    "with gfile.FastGFile(output_graph_orig, 'wb') as f:\n",
    "    f.write(output_graph_def.SerializeToString())\n",
    "\n",
    "# Run the training for as many cycles as requested on the command line.\n",
    "for i in range(how_many_training_steps):\n",
    "    # Get a batch of input bottleneck values, either calculated fresh every time\n",
    "    # with distortions applied, or from the cache stored on disk.\n",
    "    if do_distort_images:\n",
    "        train_bottlenecks, train_ground_truth = helper.get_random_distorted_bottlenecks(\n",
    "            sess, image_lists, train_batch_size, 'training',\n",
    "            images_resized_dir, distorted_jpeg_data_tensor,\n",
    "            distorted_image_tensor, resized_image_tensor, bottleneck_tensor)\n",
    "    else:\n",
    "        train_bottlenecks, train_ground_truth, _ = helper.get_random_cached_bottlenecks(\n",
    "            sess, image_lists, train_batch_size, 'training',\n",
    "            bottleneck_dir, images_resized_dir, jpeg_data_tensor,\n",
    "            bottleneck_tensor)\n",
    "    # Feed the bottlenecks and ground truth into the graph, and run a training\n",
    "    # step. Capture training summaries for TensorBoard with the `merged` op.\n",
    "    train_summary, _ = sess.run([merged, train_step],\n",
    "                                feed_dict={bottleneck_input: train_bottlenecks,\n",
    "                                           ground_truth_input: train_ground_truth})\n",
    "    train_writer.add_summary(train_summary, i)\n",
    "\n",
    "    # Every so often, print out how well the graph is training.\n",
    "    is_last_step = (i + 1 == how_many_training_steps)\n",
    "    if (i % eval_step_interval) == 0 or is_last_step:\n",
    "        train_accuracy, cross_entropy_value = sess.run(\n",
    "            [evaluation_step, cross_entropy],\n",
    "            feed_dict={bottleneck_input: train_bottlenecks,\n",
    "                       ground_truth_input: train_ground_truth})\n",
    "        print('%s: Step %d: Train accuracy = %.1f%%' % (datetime.now(), i,\n",
    "                                                      train_accuracy * 100))\n",
    "        print('%s: Step %d: Cross entropy = %f' % (datetime.now(), i,\n",
    "                                                 cross_entropy_value))\n",
    "        validation_bottlenecks, validation_ground_truth, _ = (\n",
    "            helper.get_random_cached_bottlenecks(\n",
    "                sess, image_lists, validation_batch_size, 'validation',\n",
    "                bottleneck_dir, images_resized_dir, jpeg_data_tensor,\n",
    "                bottleneck_tensor))\n",
    "        # Run a validation step and capture training summaries for TensorBoard\n",
    "        # with the `merged` op.\n",
    "        validation_summary, validation_accuracy = sess.run(\n",
    "            [merged, evaluation_step],\n",
    "            feed_dict={bottleneck_input: validation_bottlenecks,\n",
    "                     ground_truth_input: validation_ground_truth})\n",
    "        validation_writer.add_summary(validation_summary, i)\n",
    "        print('%s: Step %d: Validation accuracy = %.1f%% (N=%d)' %\n",
    "              (datetime.now(), i, validation_accuracy * 100,\n",
    "               len(validation_bottlenecks)))\n",
    "\n",
    "# We've completed all our training, so run a final test evaluation on\n",
    "# some new images we haven't used before.\n",
    "test_bottlenecks, test_ground_truth, test_filenames = (\n",
    "    helper.get_random_cached_bottlenecks(sess, image_lists, test_batch_size,\n",
    "                                         'testing', bottleneck_dir,\n",
    "                                         images_resized_dir, jpeg_data_tensor,\n",
    "                                         bottleneck_tensor))\n",
    "test_accuracy, predictions = sess.run(\n",
    "    [evaluation_step, prediction],\n",
    "    feed_dict={bottleneck_input: test_bottlenecks,\n",
    "               ground_truth_input: test_ground_truth})\n",
    "print('Final test accuracy = %.1f%% (N=%d)' % (\n",
    "    test_accuracy * 100, len(test_bottlenecks)))\n",
    "\n",
    "if print_misclassified_test_images:\n",
    "    print('=== MISCLASSIFIED TEST IMAGES ===')\n",
    "    for i, test_filename in enumerate(test_filenames):\n",
    "        if predictions[i] != test_ground_truth[i].argmax():\n",
    "            print('%70s  %s' % (test_filename, image_lists.keys()[predictions[i]]))\n",
    "\n",
    "# Write out the trained graph and labels with the weights stored as constants.\n",
    "output_graph_def = graph_util.convert_variables_to_constants(\n",
    "    sess, graph.as_graph_def(), [final_tensor_name])\n",
    "with gfile.FastGFile(output_graph, 'wb') as f:\n",
    "    f.write(output_graph_def.SerializeToString())\n",
    "with gfile.FastGFile(output_labels, 'w') as f:\n",
    "    f.write('\\n'.join(image_lists.keys()) + '\\n')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The final test accuracy is **~85%**  for our two classes **House** and **House + Pool** which is quite substantial given our training set contained approximately only ~300 images for each classes. This is where Transfer Learning really shines. All what we did was to use the trained Inception Model which already had learned basic features of lines, shapes and other features that increase in abstraction as we move towards the final layers of the model. We basically retrained the last layers where we supplied training images of Houses and House + Pools and the model using its pre-learnt features.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Want to give it a try ?\n",
    "\n",
    "We have a couple of images you can use to test the model or you can download your owns from your favorite web site."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**image-01-resized.jpeg**  <img src=\"../data/test_images/image-01.jpeg\" >\n",
    "**image-02-resized.jpeg**  <img src=\"../data/test_images/image-02.jpeg\" >\n",
    "**image-03-resized.jpeg**  <img src=\"../data/test_images/image-03.jpeg\" >\n",
    "**image-04-resized.jpeg**  <img src=\"../data/test_images/image-04.jpeg\" >\n",
    "**image-05-resized.jpeg**  <img src=\"../data/test_images/image-05.jpeg\" >\n",
    "**image-06-resized.jpeg**  <img src=\"../data/test_images/image-06.jpeg\" >\n",
    "**image-07-resized.jpeg**  <img src=\"../data/test_images/image-07.jpeg\" >\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the inference engine\n",
    "Open a terminal window form the Jupyter Notebook. <br>\n",
    "The **samples** directory includes some test images. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../doc/source/images/terminal-access.png\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run the inference engine use the following command: <br><br>\n",
    "**python test-new.py test_images/image-01.jpeg**\n",
    "<br><br>\n",
    "You can also download your own set of images and run them against the inference engine.\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the inference engine against the **original** Inception V3 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%run ../scripts/test-orig.py ../data/test_images/image-01.jpeg\n",
    "%run ../scripts/test-orig.py ../data/test_images/image-02.jpeg\n",
    "%run ../scripts/test-orig.py ../data/test_images/image-03.jpeg\n",
    "%run ../scripts/test-orig.py ../data/test_images/image-04.jpeg\n",
    "%run ../scripts/test-orig.py ../data/test_images/image-05.jpeg\n",
    "%run ../scripts/test-orig.py ../data/test_images/image-06.jpeg\n",
    "%run ../scripts/test-orig.py ../data/test_images/image-07.jpeg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the expected result as the Inception V3 model has not been trained with the new image dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the inference engine against new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%run ../scripts/test-new.py ../data/test_images/image-01.jpeg\n",
    "%run ../scripts/test-new.py ../data/test_images/image-02.jpeg\n",
    "%run ../scripts/test-new.py ../data/test_images/image-03.jpeg\n",
    "%run ../scripts/test-new.py ../data/test_images/image-04.jpeg\n",
    "%run ../scripts/test-new.py ../data/test_images/image-05.jpeg\n",
    "%run ../scripts/test-new.py ../data/test_images/image-06.jpeg\n",
    "%run ../scripts/test-new.py ../data/test_images/image-07.jpeg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yeah, the new model is able to properly classify the images..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "I hope that you are now able to apply pre-trained models to your problem statements. Be sure that the pre-trained model you have selected has been trained on a similar data set as the one that you wish to use it on. There are various architectures people have tried on different types of data sets and I strongly encourage you to go through these architectures and apply them to your own problem statements.\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
