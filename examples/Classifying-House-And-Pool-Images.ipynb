{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Licensed under the Apache License, Version 2.0 (the \"License\"); you may\n",
    "# not use this file except in compliance with the License. You may obtain\n",
    "# a copy of the License at\n",
    "#\n",
    "#      http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n",
    "# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n",
    "# License for the specific language governing permissions and limitations\n",
    "# under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning in TensorFlow with Inception V3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Transfer learning is the process of taking a pre-trained model (the weights and parameters of a network that has been trained on a large dataset by somebody else) and “fine-tuning” the model with your own dataset. The idea is that this pre-trained model will act as a feature extractor. You will remove the last layer of the network and replace it with your own classifier (depending on what your problem space is). You then freeze the weights of all the other layers and train the network normally (Freezing the layers means not changing the weights during gradient descent/optimization).\n",
    "\n",
    "For this experiment we used Google's Inception-V3 pretrained model for Image Classification. This model consists of two parts:\n",
    "    - Feature extraction part with a convolutional neural network.\n",
    "    - Classification part with fully-connected and softmax layers.\n",
    "The pre-trained Inception-v3 model achieves state-of-the-art accuracy for recognizing general objects with 1000 classes. The model extracts general features from input images in the first part and classifies them based on those features in the second part.\n",
    "\n",
    "We will use this pre-trained model and re-train it it to classify houses with or without swimming pools. \n",
    "\n",
    "The following chart shows how the data flows in the Inception v3 model, which is a Convolutional Neural Network with many layers and a complicated structure. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../doc/source/images/inception_flowchart.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In transfer learning, when you build a new model to classify your original dataset, you reuse the feature extraction part and re-train the classification part with your dataset. Since you don't have to train the feature extraction part (which is the most complex part of the model), you can train the model with less computational resources and training time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../doc/source/images/inception_transfer_learning.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "For this experiment, we built two small image datasets (less than 600 images) -- one with images of houses without swimming pools and another one with images of houses with swimming pools.\n",
    "\n",
    "After downloading the images, we took an extra step to visualize the images and remove the false positives. All the images were then saved in two different directories identifying the proper classification.\n",
    "\n",
    "In the public GitHub repo we only provided a subset of the images, but we also provided bottleneck files to represent the rest of the images in the dataset. It might be worth noting that most of the bottleneck files represent aerial view images. It would not be surprising if we recognize pools better from above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resizing\n",
    "\n",
    "The raw images need to be resized to 299 x 299. The notebook code will resize the raw images into a working directory. We're also able to reuse resized images like the one below which is stored in a folder in the repo. If you have your own large dataset, you might want to do the resize once and store the resized images to use instead of the raw images.\n",
    "\n",
    "#### For example, running the notebook resizes this image:\n",
    "\n",
    "<img src=\"../data/images/house_with_pool/house-429353_960_720.jpg\">\n",
    "\n",
    "#### To this 299 x 299 image: \n",
    "\n",
    "<img src=\"../data/images_resized/house_with_pool/house-429353_960_720.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install Python packages\n",
    "The PowerAI TensorFlow already has TensorFlow and PIL, but we need python-resize-image for the image resizing step.\n",
    "Run this cell at least once. You might need to restart your kernel after the install. Use the Kernel menu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-resize-image==1.1.11 in /opt/anaconda2/lib/python2.7/site-packages (1.1.11)\n",
      "Requirement already satisfied: pillow in /opt/anaconda2/lib/python2.7/site-packages (from python-resize-image==1.1.11) (4.2.1)\n",
      "Requirement already satisfied: requests in /opt/anaconda2/lib/python2.7/site-packages (from python-resize-image==1.1.11) (2.14.2)\n",
      "Requirement already satisfied: olefile in /opt/anaconda2/lib/python2.7/site-packages (from pillow->python-resize-image==1.1.11) (0.44)\n",
      "\u001b[31mtensorflow 1.4.0 requires tensorflow-tensorboard<0.5.0,>=0.4.0rc1, which is not installed.\u001b[0m\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 18.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install --user python-resize-image==1.1.11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports\n",
    "We put all the imports at the top of the code, because this is what most Python developers would expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "from datetime import datetime\n",
    "import os.path\n",
    "import shutil\n",
    "import sys\n",
    "\n",
    "from PIL import Image\n",
    "from resizeimage import resizeimage\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import graph_util\n",
    "from tensorflow.python.platform import gfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import image retraining function definitions\n",
    "\n",
    "The image_retraining example module from TensorFlow can be used from a notebook by importing it and calling the functions directly. A FLAGS object is used in the module. We just create one and set it in the `Parameters` section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "module_path = os.path.abspath('..')\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from image_retraining import retrain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters\n",
    "\n",
    "Many of the parameters can be changed if you choose to experiment with different images and training settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set DEBUG to True for more output\n",
    "DEBUG = False\n",
    "\n",
    "# Expect image files to always end with one of these\n",
    "JPEG_EXTENSIONS = ('.jpeg', '.JPEG', '.jpg', '.JPG')\n",
    "\n",
    "# Raw input images come from this dir in the git repo (or you can customize this to point to a new dir).\n",
    "# Only JPEG images are used. We will resize these images before using them.\n",
    "image_dir = '../data/images'\n",
    "\n",
    "# We kept some images separate for our manual testing at the end.\n",
    "test_images_dir = '../data/test_images'\n",
    "\n",
    "# If stored_images_resized, images here have already been resized are can be used w/o re-resizing\n",
    "stored_images_resized = '../data/images_resized'  # set to None to ignore\n",
    "\n",
    "# If stored_bottlenecks, supplement the image_dir collection with persisted bottlenecks from this dir\n",
    "stored_bottlenecks = '../data/bottlenecks'  # set to None to ignore\n",
    "\n",
    "# Working files are in /tmp by default\n",
    "tmp_dir = '/tmp'\n",
    "bottleneck_dir = os.path.join(tmp_dir, 'bottlenecks')\n",
    "images_resized_dir = os.path.join(tmp_dir, 'images_resized')\n",
    "summaries_dir = os.path.join(tmp_dir, 'retrain_logs')\n",
    "\n",
    "# Download the original inception model to/from here\n",
    "model_dir = os.path.join(tmp_dir, 'inception')\n",
    "inception_url = 'http://download.tensorflow.org/models/image/imagenet/inception-2015-12-05.tgz'\n",
    "\n",
    "# Store the graph before and after training\n",
    "output_graph_orig = \"output_graph_orig.pb\"\n",
    "output_graph = \"output_graph.pb\"\n",
    "output_labels = \"output_labels.txt\"\n",
    "\n",
    "# Training params\n",
    "architecture = 'inception_v3'\n",
    "final_tensor_name = \"final_result\"\n",
    "how_many_training_steps = 500\n",
    "learning_rate = 0.01\n",
    "testing_percentage = 10\n",
    "validation_percentage = 10\n",
    "eval_step_interval = 10\n",
    "train_batch_size = 100\n",
    "test_batch_size = -1\n",
    "validation_batch_size = 100\n",
    "print_misclassified_test_images = False\n",
    "\n",
    "# Since we are using persisted bottleneck files, we won't play with distortion.\n",
    "# Distortion would have limited impact with our small set of image files.\n",
    "flip_left_right = False\n",
    "random_crop = 0\n",
    "random_scale = 0\n",
    "random_brightness = 0\n",
    "\n",
    "# Download once and re-use by default\n",
    "force_inception_download = False\n",
    "\n",
    "# Create a FLAGS object with these attributes\n",
    "FLAGS = type('FlagsObject', (object,), {\n",
    "    'architecture': architecture,\n",
    "    'model_dir': model_dir,\n",
    "    'intermediate_store_frequency': 0,\n",
    "    'summaries_dir': summaries_dir,\n",
    "    'learning_rate': learning_rate,\n",
    "    'image_dir': images_resized_dir,\n",
    "    'testing_percentage': testing_percentage,\n",
    "    'validation_percentage': validation_percentage,\n",
    "    'random_scale': random_scale,\n",
    "    'random_crop': random_crop,\n",
    "    'flip_left_right': flip_left_right,\n",
    "    'random_brightness': random_brightness,\n",
    "    'bottleneck_dir': bottleneck_dir,\n",
    "    'final_tensor_name': final_tensor_name,\n",
    "    'how_many_training_steps': how_many_training_steps,\n",
    "    'train_batch_size': train_batch_size,\n",
    "    'test_batch_size': test_batch_size,\n",
    "    'eval_step_interval': eval_step_interval,\n",
    "    'validation_batch_size': validation_batch_size,\n",
    "    'print_misclassified_test_images': print_misclassified_test_images,\n",
    "    'output_graph': output_graph,\n",
    "    'output_labels': output_labels\n",
    "})\n",
    "\n",
    "# Setting the FLAGS in retrain allows us to call the functions directly\n",
    "retrain.FLAGS = FLAGS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download the Inception model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Downloading inception-2015-12-05.tgz 100.0%\n",
      "Successfully downloaded inception-2015-12-05.tgz 88931400 bytes.\n"
     ]
    }
   ],
   "source": [
    "# Download the Inception model once and reuse it (set the flag and clobber it each time).\n",
    "if force_inception_download and os.path.isdir(model_dir):    \n",
    "    shutil.rmtree(model_dir)\n",
    "retrain.maybe_download_and_extract()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the images\n",
    "\n",
    "The Inception model requires 299 X 299 pixel sizes.\n",
    "First copy the files from `stored_images_resized` into `images_resized_dir`.\n",
    "With these stored images that are already resized, we don't need to repeat the process.\n",
    "Next copy and resize the remaining raw images from `image_dir` into `images_resized_dir`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def resize_images(src_dir, dest_dir):\n",
    "    if not os.path.isdir(src_dir):\n",
    "        raise Exception(src_dir + \" is not a directory\")\n",
    "    if not os.path.exists(dest_dir):\n",
    "        os.mkdir(dest_dir)\n",
    "\n",
    "    raw_images = {image for image in os.listdir(src_dir) if image.endswith(\n",
    "        JPEG_EXTENSIONS)}\n",
    "    dest_images = {image for image in os.listdir(dest_dir)}\n",
    "\n",
    "    # Resize the ones that are not already in the dest dir\n",
    "    for image in raw_images - dest_images:\n",
    "        if DEBUG:\n",
    "            print(\"Resizing \" + image)\n",
    "        resize_image(image, src_dir, dest_dir)\n",
    "\n",
    "\n",
    "def resize_image(image_file, src_dir, dest_dir):\n",
    "    in_file = os.path.join(src_dir, image_file)\n",
    "    with open(in_file, 'r+b') as fd_img:\n",
    "        with Image.open(fd_img) as img:\n",
    "            resized_image = resizeimage.resize_contain(\n",
    "                img, [299, 299]).convert(\"RGB\")\n",
    "            resized_image.save(os.path.join(dest_dir, image_file), img.format)\n",
    "\n",
    "# Use a fresh working dir for the resized images\n",
    "if os.path.isdir(images_resized_dir):\n",
    "    shutil.rmtree(images_resized_dir)\n",
    "os.mkdir(images_resized_dir)\n",
    "    \n",
    "subdirs = ('house_with_pool', 'house_without_pool')\n",
    "\n",
    "# Copy in the image files\n",
    "for subdir in subdirs:\n",
    "    dest_dir = os.path.join(images_resized_dir, subdir)\n",
    "    if not os.path.isdir(dest_dir):\n",
    "        os.mkdir(dest_dir)\n",
    "      \n",
    "    # Copy the already resized files first, if any, from the repo or a custom dir\n",
    "    if stored_images_resized:\n",
    "        source_dir = os.path.join(stored_images_resized, subdir)\n",
    "        if os.path.isdir(source_dir):\n",
    "            for f in os.listdir(source_dir):\n",
    "                path = os.path.join(source_dir, f)\n",
    "                if (os.path.isfile(path)):\n",
    "                    shutil.copy(path, dest_dir)\n",
    "                    \n",
    "    # Copy/resize the remaining raw images into the images_resized_dir(s)\n",
    "    resize_images(os.path.join(image_dir, subdir), dest_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize dataset images\n",
    "\n",
    "Since Jupyter notebooks are great at showing markdown documentation as well as code and output, we can look at some of the images here.\n",
    "\n",
    "To visualize a different image, double click on the displayed image below, the markdown text will show up. Change the image file name to display another one.\n",
    "\n",
    "Some false positives have been removed from our dataset, but it is still interesting to see which images are harder to classify. Lakes and ponds would be something to look into. Some of the lower confidence numbers seem to come from shapes that resemble pools (and not bodies of water).\n",
    "\n",
    "Removing false positives can often help the training but if we want to improve the training to classify those images with confidence as well, then we might just need a bigger dataset with a good amount of relevant examples to learn from.\n",
    "\n",
    "<img src=\"../data/images/house_without_pool/giethoorn-2368494__340.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Copy stored bottleneck files\n",
    "Many previously calculated bottleneck files are stored in `stored_bottlenecks`\n",
    "to improve our dataset size and reduce processing time. Here we copy them into the working `bottleneck_dir`.\n",
    "We also create a placeholder image file so that they are included in our image lists for training, validation,\n",
    "and testing. The placeholder contents won't be used because the bottleneck is used instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a fresh working dir for the bottleneck files  \n",
    "if os.path.isdir(bottleneck_dir):    \n",
    "    shutil.rmtree(bottleneck_dir)\n",
    "os.mkdir(bottleneck_dir)\n",
    "\n",
    "subdirs = ('house_with_pool', 'house_without_pool')\n",
    "\n",
    "# Copy in the stored bottleneck files\n",
    "for subdir in subdirs:\n",
    "    dest_dir = os.path.join(bottleneck_dir, subdir)\n",
    "    if not os.path.isdir(dest_dir):\n",
    "        os.mkdir(dest_dir)\n",
    "\n",
    "    image_dest_dir = os.path.join(images_resized_dir, subdir)\n",
    "\n",
    "    if stored_bottlenecks:\n",
    "        source_dir = os.path.join(stored_bottlenecks, subdir)\n",
    "        if os.path.isdir(source_dir):\n",
    "            for f in os.listdir(source_dir):\n",
    "                path = os.path.join(source_dir, f)\n",
    "                if (os.path.isfile(path)):\n",
    "                    # Copy the persisted bottleneck to bottlenecks dir\n",
    "                    shutil.copy(path, dest_dir)\n",
    "                    # \"touch\" the file (w/o the .txt) to create a placeholder image\n",
    "                    # This image file will only be used to build the lists.\n",
    "                    if DEBUG:\n",
    "                        print(\"Creating placeholder image at %s\" % os.path.join(image_dest_dir, f[:-4]))\n",
    "                    open(os.path.join(image_dest_dir, f[:-4]), 'a').close\n",
    "                        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retraining\n",
    "\n",
    "The following code demonstrates how to take an Inception v3 architecture model trained on\n",
    "ImageNet images, and train a new top layer that can recognize other classes of\n",
    "images.\n",
    "\n",
    "The top layer receives as input a 2048-dimensional vector for each image. We\n",
    "train a softmax layer on top of this representation. Assuming the softmax layer\n",
    "contains N labels, this corresponds to learning N + 2048*N model parameters\n",
    "corresponding to the learned biases and weights.\n",
    "\n",
    "We have a folder with two subfolders called **house_with_pool** and **house_without_pool**.\n",
    "JPEG images have been selected for training and placed in the proper folder.\n",
    "The subfolder names are important, since they define what label is applied to each image, but the filenames themselves don't matter. The label for each image is taken from the name of the subfolder it's in. This produces a new model file that can be loaded and run by any TensorFlow program.\n",
    "\n",
    "In addition to the small sample of images, we have a larger set of bottlenecks. These were captured from\n",
    "images used in earlier runs and will be used here to increase the size of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for images in 'house_with_pool'\n",
      "Looking for images in 'house_without_pool'\n",
      "Creating bottleneck at /tmp/bottlenecks/house_without_pool/home-2436063__340.jpg.txt\n",
      "Creating bottleneck at /tmp/bottlenecks/house_without_pool/house-2414374__340.jpg.txt\n",
      "Creating bottleneck at /tmp/bottlenecks/house_without_pool/architecture-1853334__340.jpg.txt\n",
      "Creating bottleneck at /tmp/bottlenecks/house_without_pool/house-2418106__340.jpg.txt\n",
      "Creating bottleneck at /tmp/bottlenecks/house_without_pool/house-2187170__340.jpg.txt\n",
      "Creating bottleneck at /tmp/bottlenecks/house_without_pool/giethoorn-2368494__340.jpg.txt\n",
      "Creating bottleneck at /tmp/bottlenecks/house_without_pool/new-home-2416180__340.jpg.txt\n",
      "Creating bottleneck at /tmp/bottlenecks/house_without_pool/beautiful-2178398__340.jpg.txt\n",
      "Creating bottleneck at /tmp/bottlenecks/house_without_pool/architecture-1836607__340.jpg.txt\n",
      "Creating bottleneck at /tmp/bottlenecks/house_without_pool/hut-202035__340.jpg.txt\n",
      "Creating bottleneck at /tmp/bottlenecks/house_without_pool/home-2409041__340.jpg.txt\n",
      "Creating bottleneck at /tmp/bottlenecks/house_without_pool/old-2437486__340.jpg.txt\n",
      "Creating bottleneck at /tmp/bottlenecks/house_without_pool/house-2469110__340.jpg.txt\n",
      "Creating bottleneck at /tmp/bottlenecks/house_without_pool/home-2409037__340.jpg.txt\n",
      "Creating bottleneck at /tmp/bottlenecks/house_without_pool/house-2280072__340.jpg.txt\n",
      "Creating bottleneck at /tmp/bottlenecks/house_without_pool/luxury-home-2412139__340.jpg.txt\n",
      "Creating bottleneck at /tmp/bottlenecks/house_without_pool/house-2417321__340.jpg.txt\n",
      "Creating bottleneck at /tmp/bottlenecks/house_without_pool/architecture-1867426__340.jpg.txt\n",
      "Creating bottleneck at /tmp/bottlenecks/house_without_pool/house-2417417__340.jpg.txt\n",
      "Creating bottleneck at /tmp/bottlenecks/house_without_pool/house-2417271__340.jpg.txt\n",
      "100 bottleneck files created.\n",
      "Creating bottleneck at /tmp/bottlenecks/house_without_pool/pexels-photo-277667.jpeg.txt\n",
      "Creating bottleneck at /tmp/bottlenecks/house_without_pool/nature-1547302__340.jpg.txt\n",
      "Creating bottleneck at /tmp/bottlenecks/house_without_pool/hut-209466__340.jpg.txt\n",
      "Creating bottleneck at /tmp/bottlenecks/house_without_pool/house-2469067__340.jpg.txt\n",
      "Creating bottleneck at /tmp/bottlenecks/house_with_pool/villa-267335__340.jpg.txt\n",
      "Creating bottleneck at /tmp/bottlenecks/house_with_pool/pexels-photo.jpg.txt\n",
      "Creating bottleneck at /tmp/bottlenecks/house_with_pool/pool-237990__340.jpg.txt\n",
      "Creating bottleneck at /tmp/bottlenecks/house_with_pool/house-429353_960_720.jpg.txt\n",
      "Creating bottleneck at /tmp/bottlenecks/house_with_pool/clouds-803886__340.jpg.txt\n",
      "Creating bottleneck at /tmp/bottlenecks/house_with_pool/holiday-house-64393__340.jpg.txt\n",
      "Creating bottleneck at /tmp/bottlenecks/house_with_pool/design-2411846__340.jpg.txt\n",
      "Creating bottleneck at /tmp/bottlenecks/house_with_pool/pool-681891__340.jpg.txt\n",
      "Creating bottleneck at /tmp/bottlenecks/house_with_pool/pool-1148213__340.jpg.txt\n",
      "Creating bottleneck at /tmp/bottlenecks/house_with_pool/pool-1367391__340.jpg.txt\n",
      "200 bottleneck files created.\n",
      "300 bottleneck files created.\n",
      "400 bottleneck files created.\n",
      "Creating bottleneck at /tmp/bottlenecks/house_with_pool/holiday-house-and-pool-kenya-africa.jpg.txt\n",
      "500 bottleneck files created.\n",
      "Creating bottleneck at /tmp/bottlenecks/house_with_pool/swimming-828795__340.jpg.txt\n",
      "Creating bottleneck at /tmp/bottlenecks/house_with_pool/manor-house-2359884__340.jpg.txt\n",
      "Creating bottleneck at /tmp/bottlenecks/house_with_pool/pool-691008__340.jpg.txt\n",
      "Creating bottleneck at /tmp/bottlenecks/house_with_pool/swimming-pool-2359638__340.jpg.txt\n",
      "Creating bottleneck at /tmp/bottlenecks/house_with_pool/pool-238012__340.jpg.txt\n",
      "INFO:tensorflow:Froze 2 variables.\n",
      "Converted 2 variables to const ops.\n",
      "2018-11-29 00:47:33.467346: Step 0: Train accuracy = 52.0%\n",
      "2018-11-29 00:47:33.467650: Step 0: Cross entropy = 0.668907\n",
      "2018-11-29 00:47:33.791634: Step 0: Validation accuracy = 46.0% (N=100)\n",
      "2018-11-29 00:47:35.002741: Step 10: Train accuracy = 87.0%\n",
      "2018-11-29 00:47:35.002978: Step 10: Cross entropy = 0.556158\n",
      "2018-11-29 00:47:35.125698: Step 10: Validation accuracy = 76.0% (N=100)\n",
      "2018-11-29 00:47:36.199225: Step 20: Train accuracy = 83.0%\n",
      "2018-11-29 00:47:36.199593: Step 20: Cross entropy = 0.469544\n",
      "2018-11-29 00:47:36.292921: Step 20: Validation accuracy = 81.0% (N=100)\n",
      "2018-11-29 00:47:37.359320: Step 30: Train accuracy = 82.0%\n",
      "2018-11-29 00:47:37.359558: Step 30: Cross entropy = 0.447938\n",
      "2018-11-29 00:47:37.453027: Step 30: Validation accuracy = 71.0% (N=100)\n",
      "2018-11-29 00:47:38.702849: Step 40: Train accuracy = 87.0%\n",
      "2018-11-29 00:47:38.703193: Step 40: Cross entropy = 0.454858\n",
      "2018-11-29 00:47:38.832161: Step 40: Validation accuracy = 71.0% (N=100)\n",
      "2018-11-29 00:47:39.942004: Step 50: Train accuracy = 87.0%\n",
      "2018-11-29 00:47:39.942528: Step 50: Cross entropy = 0.407271\n",
      "2018-11-29 00:47:40.036591: Step 50: Validation accuracy = 71.0% (N=100)\n",
      "2018-11-29 00:47:41.014666: Step 60: Train accuracy = 87.0%\n",
      "2018-11-29 00:47:41.015023: Step 60: Cross entropy = 0.379498\n",
      "2018-11-29 00:47:41.108840: Step 60: Validation accuracy = 84.0% (N=100)\n",
      "2018-11-29 00:47:42.056912: Step 70: Train accuracy = 87.0%\n",
      "2018-11-29 00:47:42.057256: Step 70: Cross entropy = 0.397868\n",
      "2018-11-29 00:47:42.150006: Step 70: Validation accuracy = 85.0% (N=100)\n",
      "2018-11-29 00:47:43.105903: Step 80: Train accuracy = 81.0%\n",
      "2018-11-29 00:47:43.106222: Step 80: Cross entropy = 0.435889\n",
      "2018-11-29 00:47:43.199288: Step 80: Validation accuracy = 86.0% (N=100)\n",
      "2018-11-29 00:47:44.156697: Step 90: Train accuracy = 95.0%\n",
      "2018-11-29 00:47:44.157005: Step 90: Cross entropy = 0.330027\n",
      "2018-11-29 00:47:44.250495: Step 90: Validation accuracy = 72.0% (N=100)\n",
      "2018-11-29 00:47:45.205683: Step 100: Train accuracy = 85.0%\n",
      "2018-11-29 00:47:45.205949: Step 100: Cross entropy = 0.349125\n",
      "2018-11-29 00:47:45.298533: Step 100: Validation accuracy = 81.0% (N=100)\n",
      "2018-11-29 00:47:46.249009: Step 110: Train accuracy = 89.0%\n",
      "2018-11-29 00:47:46.249164: Step 110: Cross entropy = 0.291437\n",
      "2018-11-29 00:47:46.342887: Step 110: Validation accuracy = 80.0% (N=100)\n",
      "2018-11-29 00:47:47.299517: Step 120: Train accuracy = 88.0%\n",
      "2018-11-29 00:47:47.299730: Step 120: Cross entropy = 0.358038\n",
      "2018-11-29 00:47:47.392947: Step 120: Validation accuracy = 82.0% (N=100)\n",
      "2018-11-29 00:47:48.381986: Step 130: Train accuracy = 96.0%\n",
      "2018-11-29 00:47:48.382383: Step 130: Cross entropy = 0.267439\n",
      "2018-11-29 00:47:48.475154: Step 130: Validation accuracy = 78.0% (N=100)\n",
      "2018-11-29 00:47:49.429050: Step 140: Train accuracy = 91.0%\n",
      "2018-11-29 00:47:49.429403: Step 140: Cross entropy = 0.278927\n",
      "2018-11-29 00:47:49.522175: Step 140: Validation accuracy = 80.0% (N=100)\n",
      "2018-11-29 00:47:50.481923: Step 150: Train accuracy = 89.0%\n",
      "2018-11-29 00:47:50.482329: Step 150: Cross entropy = 0.298874\n",
      "2018-11-29 00:47:50.576947: Step 150: Validation accuracy = 83.0% (N=100)\n",
      "2018-11-29 00:47:51.531361: Step 160: Train accuracy = 92.0%\n",
      "2018-11-29 00:47:51.531715: Step 160: Cross entropy = 0.299249\n",
      "2018-11-29 00:47:51.626078: Step 160: Validation accuracy = 76.0% (N=100)\n",
      "2018-11-29 00:47:52.577372: Step 170: Train accuracy = 90.0%\n",
      "2018-11-29 00:47:52.577814: Step 170: Cross entropy = 0.260256\n",
      "2018-11-29 00:47:52.670916: Step 170: Validation accuracy = 82.0% (N=100)\n",
      "2018-11-29 00:47:53.624319: Step 180: Train accuracy = 94.0%\n",
      "2018-11-29 00:47:53.624644: Step 180: Cross entropy = 0.222513\n",
      "2018-11-29 00:47:53.717423: Step 180: Validation accuracy = 85.0% (N=100)\n",
      "2018-11-29 00:47:54.670144: Step 190: Train accuracy = 93.0%\n",
      "2018-11-29 00:47:54.670403: Step 190: Cross entropy = 0.269346\n",
      "2018-11-29 00:47:54.763287: Step 190: Validation accuracy = 83.0% (N=100)\n",
      "2018-11-29 00:47:55.713734: Step 200: Train accuracy = 96.0%\n",
      "2018-11-29 00:47:55.714126: Step 200: Cross entropy = 0.247573\n",
      "2018-11-29 00:47:55.806819: Step 200: Validation accuracy = 85.0% (N=100)\n",
      "2018-11-29 00:47:56.761641: Step 210: Train accuracy = 94.0%\n",
      "2018-11-29 00:47:56.762123: Step 210: Cross entropy = 0.239625\n",
      "2018-11-29 00:47:56.858227: Step 210: Validation accuracy = 82.0% (N=100)\n",
      "2018-11-29 00:47:57.807641: Step 220: Train accuracy = 90.0%\n",
      "2018-11-29 00:47:57.807901: Step 220: Cross entropy = 0.333667\n",
      "2018-11-29 00:47:57.901127: Step 220: Validation accuracy = 76.0% (N=100)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-29 00:47:58.855715: Step 230: Train accuracy = 94.0%\n",
      "2018-11-29 00:47:58.855975: Step 230: Cross entropy = 0.267351\n",
      "2018-11-29 00:47:58.948999: Step 230: Validation accuracy = 82.0% (N=100)\n",
      "2018-11-29 00:47:59.901495: Step 240: Train accuracy = 91.0%\n",
      "2018-11-29 00:47:59.901760: Step 240: Cross entropy = 0.280648\n",
      "2018-11-29 00:47:59.994721: Step 240: Validation accuracy = 85.0% (N=100)\n",
      "2018-11-29 00:48:00.947206: Step 250: Train accuracy = 90.0%\n",
      "2018-11-29 00:48:00.947595: Step 250: Cross entropy = 0.275768\n",
      "2018-11-29 00:48:01.040538: Step 250: Validation accuracy = 88.0% (N=100)\n",
      "2018-11-29 00:48:01.994391: Step 260: Train accuracy = 96.0%\n",
      "2018-11-29 00:48:01.994658: Step 260: Cross entropy = 0.210579\n",
      "2018-11-29 00:48:02.087487: Step 260: Validation accuracy = 81.0% (N=100)\n",
      "2018-11-29 00:48:03.043120: Step 270: Train accuracy = 92.0%\n",
      "2018-11-29 00:48:03.043505: Step 270: Cross entropy = 0.253348\n",
      "2018-11-29 00:48:03.136341: Step 270: Validation accuracy = 80.0% (N=100)\n",
      "2018-11-29 00:48:04.092232: Step 280: Train accuracy = 94.0%\n",
      "2018-11-29 00:48:04.092667: Step 280: Cross entropy = 0.229542\n",
      "2018-11-29 00:48:04.185830: Step 280: Validation accuracy = 74.0% (N=100)\n",
      "2018-11-29 00:48:05.135809: Step 290: Train accuracy = 97.0%\n",
      "2018-11-29 00:48:05.136099: Step 290: Cross entropy = 0.186495\n",
      "2018-11-29 00:48:05.229124: Step 290: Validation accuracy = 73.0% (N=100)\n",
      "2018-11-29 00:48:06.184384: Step 300: Train accuracy = 96.0%\n",
      "2018-11-29 00:48:06.184635: Step 300: Cross entropy = 0.206210\n",
      "2018-11-29 00:48:06.277686: Step 300: Validation accuracy = 83.0% (N=100)\n",
      "2018-11-29 00:48:07.228065: Step 310: Train accuracy = 95.0%\n",
      "2018-11-29 00:48:07.228261: Step 310: Cross entropy = 0.200246\n",
      "2018-11-29 00:48:07.321230: Step 310: Validation accuracy = 78.0% (N=100)\n",
      "2018-11-29 00:48:08.269941: Step 320: Train accuracy = 95.0%\n",
      "2018-11-29 00:48:08.270236: Step 320: Cross entropy = 0.231805\n",
      "2018-11-29 00:48:08.363315: Step 320: Validation accuracy = 80.0% (N=100)\n",
      "2018-11-29 00:48:09.312878: Step 330: Train accuracy = 93.0%\n",
      "2018-11-29 00:48:09.313250: Step 330: Cross entropy = 0.218936\n",
      "2018-11-29 00:48:09.406000: Step 330: Validation accuracy = 74.0% (N=100)\n",
      "2018-11-29 00:48:10.362976: Step 340: Train accuracy = 92.0%\n",
      "2018-11-29 00:48:10.363331: Step 340: Cross entropy = 0.234068\n",
      "2018-11-29 00:48:10.456750: Step 340: Validation accuracy = 85.0% (N=100)\n",
      "2018-11-29 00:48:11.407633: Step 350: Train accuracy = 96.0%\n",
      "2018-11-29 00:48:11.407933: Step 350: Cross entropy = 0.225221\n",
      "2018-11-29 00:48:11.508791: Step 350: Validation accuracy = 78.0% (N=100)\n",
      "2018-11-29 00:48:12.459293: Step 360: Train accuracy = 95.0%\n",
      "2018-11-29 00:48:12.459497: Step 360: Cross entropy = 0.213671\n",
      "2018-11-29 00:48:12.551773: Step 360: Validation accuracy = 76.0% (N=100)\n",
      "2018-11-29 00:48:13.501821: Step 370: Train accuracy = 97.0%\n",
      "2018-11-29 00:48:13.502162: Step 370: Cross entropy = 0.185453\n",
      "2018-11-29 00:48:13.598612: Step 370: Validation accuracy = 88.0% (N=100)\n",
      "2018-11-29 00:48:14.548085: Step 380: Train accuracy = 97.0%\n",
      "2018-11-29 00:48:14.548438: Step 380: Cross entropy = 0.187207\n",
      "2018-11-29 00:48:14.643528: Step 380: Validation accuracy = 80.0% (N=100)\n",
      "2018-11-29 00:48:15.595236: Step 390: Train accuracy = 94.0%\n",
      "2018-11-29 00:48:15.595602: Step 390: Cross entropy = 0.196060\n",
      "2018-11-29 00:48:15.688579: Step 390: Validation accuracy = 85.0% (N=100)\n",
      "2018-11-29 00:48:16.644368: Step 400: Train accuracy = 95.0%\n",
      "2018-11-29 00:48:16.644739: Step 400: Cross entropy = 0.220419\n",
      "2018-11-29 00:48:16.738118: Step 400: Validation accuracy = 89.0% (N=100)\n",
      "2018-11-29 00:48:17.690668: Step 410: Train accuracy = 90.0%\n",
      "2018-11-29 00:48:17.691084: Step 410: Cross entropy = 0.282516\n",
      "2018-11-29 00:48:17.783866: Step 410: Validation accuracy = 78.0% (N=100)\n",
      "2018-11-29 00:48:18.736885: Step 420: Train accuracy = 96.0%\n",
      "2018-11-29 00:48:18.737420: Step 420: Cross entropy = 0.201520\n",
      "2018-11-29 00:48:18.833254: Step 420: Validation accuracy = 76.0% (N=100)\n",
      "2018-11-29 00:48:19.784901: Step 430: Train accuracy = 99.0%\n",
      "2018-11-29 00:48:19.785038: Step 430: Cross entropy = 0.150657\n",
      "2018-11-29 00:48:19.878262: Step 430: Validation accuracy = 86.0% (N=100)\n",
      "2018-11-29 00:48:20.833047: Step 440: Train accuracy = 97.0%\n",
      "2018-11-29 00:48:20.833523: Step 440: Cross entropy = 0.190909\n",
      "2018-11-29 00:48:20.926319: Step 440: Validation accuracy = 75.0% (N=100)\n",
      "2018-11-29 00:48:21.873011: Step 450: Train accuracy = 99.0%\n",
      "2018-11-29 00:48:21.873279: Step 450: Cross entropy = 0.162693\n",
      "2018-11-29 00:48:21.966292: Step 450: Validation accuracy = 72.0% (N=100)\n",
      "2018-11-29 00:48:22.919008: Step 460: Train accuracy = 96.0%\n",
      "2018-11-29 00:48:22.919378: Step 460: Cross entropy = 0.193601\n",
      "2018-11-29 00:48:23.012375: Step 460: Validation accuracy = 82.0% (N=100)\n",
      "2018-11-29 00:48:23.966221: Step 470: Train accuracy = 92.0%\n",
      "2018-11-29 00:48:23.966664: Step 470: Cross entropy = 0.247672\n",
      "2018-11-29 00:48:24.059471: Step 470: Validation accuracy = 88.0% (N=100)\n",
      "2018-11-29 00:48:25.015730: Step 480: Train accuracy = 97.0%\n",
      "2018-11-29 00:48:25.016003: Step 480: Cross entropy = 0.175364\n",
      "2018-11-29 00:48:25.109157: Step 480: Validation accuracy = 84.0% (N=100)\n",
      "2018-11-29 00:48:26.057059: Step 490: Train accuracy = 92.0%\n",
      "2018-11-29 00:48:26.057304: Step 490: Cross entropy = 0.218389\n",
      "2018-11-29 00:48:26.150442: Step 490: Validation accuracy = 86.0% (N=100)\n",
      "2018-11-29 00:48:27.011420: Step 499: Train accuracy = 98.0%\n",
      "2018-11-29 00:48:27.011723: Step 499: Cross entropy = 0.171931\n",
      "2018-11-29 00:48:27.104621: Step 499: Validation accuracy = 84.0% (N=100)\n",
      "Final test accuracy = 84.6% (N=52)\n",
      "INFO:tensorflow:Froze 2 variables.\n",
      "Converted 2 variables to const ops.\n"
     ]
    }
   ],
   "source": [
    "  # Setup the directory we'll write summaries to for TensorBoard\n",
    "  if tf.gfile.Exists(FLAGS.summaries_dir):\n",
    "    tf.gfile.DeleteRecursively(FLAGS.summaries_dir)\n",
    "  tf.gfile.MakeDirs(FLAGS.summaries_dir)\n",
    "\n",
    "  # Set up the pre-trained graph.\n",
    "  graph, bottleneck_tensor, jpeg_data_tensor, resized_image_tensor = (\n",
    "      retrain.create_inception_graph())\n",
    "\n",
    "  # Look at the folder structure, and create lists of all the images.\n",
    "  # This is why we use placeholder images when we reuse bottleneck files.\n",
    "  image_lists = retrain.create_image_lists(FLAGS.image_dir, FLAGS.testing_percentage,\n",
    "                                   FLAGS.validation_percentage)\n",
    "  class_count = len(image_lists.keys())\n",
    "  if class_count == 0:\n",
    "    raise Exception('No valid folders of images found at ' + FLAGS.image_dir)\n",
    "  if class_count == 1:\n",
    "    raise Exception('Only one valid folder of images found at ' + FLAGS.image_dir +\n",
    "          ' - multiple classes are needed for classification.')\n",
    "\n",
    "  with tf.Session(graph=graph) as sess:\n",
    "\n",
    "    # Calculate and cache bottleneck files based on the resized images\n",
    "    retrain.cache_bottlenecks(sess, image_lists, FLAGS.image_dir,\n",
    "                    FLAGS.bottleneck_dir, jpeg_data_tensor,\n",
    "                    bottleneck_tensor)\n",
    "\n",
    "    # Add the new layer that we'll be training.\n",
    "    (train_step, cross_entropy, bottleneck_input, ground_truth_input,\n",
    "     final_tensor) = retrain.add_final_training_ops(len(image_lists.keys()),\n",
    "                                            FLAGS.final_tensor_name,\n",
    "                                            bottleneck_tensor)\n",
    "\n",
    "    # Create the operations we need to evaluate the accuracy of our new layer.\n",
    "    evaluation_step, prediction = retrain.add_evaluation_step(\n",
    "        final_tensor, ground_truth_input)\n",
    "\n",
    "    # Merge all the summaries and write them out to the summaries_dir\n",
    "    merged = tf.summary.merge_all()\n",
    "    train_writer = tf.summary.FileWriter(FLAGS.summaries_dir + '/train',\n",
    "                                         sess.graph)\n",
    "\n",
    "    validation_writer = tf.summary.FileWriter(\n",
    "        FLAGS.summaries_dir + '/validation')\n",
    "\n",
    "    # Set up all our weights to their initial default values.\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    \n",
    "    # Save the original graph, so we can compare results later!\n",
    "    output_graph_def = graph_util.convert_variables_to_constants(\n",
    "        sess, graph.as_graph_def(), [final_tensor_name])\n",
    "    with gfile.FastGFile(output_graph_orig, 'wb') as f:\n",
    "        f.write(output_graph_def.SerializeToString())\n",
    "\n",
    "    # Run the training!\n",
    "    for i in range(FLAGS.how_many_training_steps):\n",
    "\n",
    "      (train_bottlenecks, train_ground_truth, _) = retrain.get_random_cached_bottlenecks(\n",
    "             sess, image_lists, FLAGS.train_batch_size, 'training',\n",
    "             FLAGS.bottleneck_dir, FLAGS.image_dir, jpeg_data_tensor,\n",
    "             bottleneck_tensor)\n",
    "    \n",
    "      # Feed the bottlenecks and ground truth into the graph, and run a training\n",
    "      # step. Capture training summaries for TensorBoard with the `merged` op.\n",
    "      train_summary, _ = sess.run(\n",
    "          [merged, train_step],\n",
    "          feed_dict={bottleneck_input: train_bottlenecks,\n",
    "                     ground_truth_input: train_ground_truth})\n",
    "      train_writer.add_summary(train_summary, i)\n",
    "\n",
    "      # Every so often, print out how well the graph is training.\n",
    "      is_last_step = (i + 1 == FLAGS.how_many_training_steps)\n",
    "      if (i % FLAGS.eval_step_interval) == 0 or is_last_step:\n",
    "        train_accuracy, cross_entropy_value = sess.run(\n",
    "            [evaluation_step, cross_entropy],\n",
    "            feed_dict={bottleneck_input: train_bottlenecks,\n",
    "                       ground_truth_input: train_ground_truth})\n",
    "        print('%s: Step %d: Train accuracy = %.1f%%' % (datetime.now(), i,\n",
    "                                                        train_accuracy * 100))\n",
    "        print('%s: Step %d: Cross entropy = %f' % (datetime.now(), i,\n",
    "                                                   cross_entropy_value))\n",
    "        validation_bottlenecks, validation_ground_truth, _ = (\n",
    "            retrain.get_random_cached_bottlenecks(\n",
    "                sess, image_lists, FLAGS.validation_batch_size, 'validation',\n",
    "                FLAGS.bottleneck_dir, FLAGS.image_dir, jpeg_data_tensor,\n",
    "                bottleneck_tensor))\n",
    "        # Run a validation step and capture training summaries for TensorBoard\n",
    "        # with the `merged` op.\n",
    "        validation_summary, validation_accuracy = sess.run(\n",
    "            [merged, evaluation_step],\n",
    "            feed_dict={bottleneck_input: validation_bottlenecks,\n",
    "                       ground_truth_input: validation_ground_truth})\n",
    "        validation_writer.add_summary(validation_summary, i)\n",
    "        print('%s: Step %d: Validation accuracy = %.1f%% (N=%d)' %\n",
    "              (datetime.now(), i, validation_accuracy * 100,\n",
    "               len(validation_bottlenecks)))\n",
    "\n",
    "    # We've completed all our training, so run a final test evaluation on\n",
    "    # some new images we haven't used before.\n",
    "    test_bottlenecks, test_ground_truth, test_filenames = (\n",
    "        retrain.get_random_cached_bottlenecks(sess, image_lists, FLAGS.test_batch_size,\n",
    "                                      'testing', FLAGS.bottleneck_dir,\n",
    "                                      FLAGS.image_dir, jpeg_data_tensor,\n",
    "                                      bottleneck_tensor))\n",
    "    test_accuracy, predictions = sess.run(\n",
    "        [evaluation_step, prediction],\n",
    "        feed_dict={bottleneck_input: test_bottlenecks,\n",
    "                   ground_truth_input: test_ground_truth})\n",
    "    print('Final test accuracy = %.1f%% (N=%d)' % (\n",
    "        test_accuracy * 100, len(test_bottlenecks)))\n",
    "\n",
    "    if FLAGS.print_misclassified_test_images:\n",
    "      print('=== MISCLASSIFIED TEST IMAGES ===')\n",
    "      for i, test_filename in enumerate(test_filenames):\n",
    "        if predictions[i] != test_ground_truth[i].argmax():\n",
    "          print('%70s  %s' % (test_filename,\n",
    "                              list(image_lists.keys())[predictions[i]]))\n",
    "\n",
    "    # Write out the trained graph and labels with the weights stored as\n",
    "    # constants.\n",
    "    output_graph_def = graph_util.convert_variables_to_constants(\n",
    "        sess, graph.as_graph_def(), [FLAGS.final_tensor_name])\n",
    "    with gfile.FastGFile(FLAGS.output_graph, 'wb') as f:\n",
    "      f.write(output_graph_def.SerializeToString())\n",
    "    with gfile.FastGFile(FLAGS.output_labels, 'w') as f:\n",
    "      f.write('\\n'.join(image_lists.keys()) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The final test accuracy is **~85%** for our two classes **house_with_pool** and **house_without_pool** which is quite substantial given our training set contained less than 600 images. This is where Transfer Learning really shines. We used the trained Inception Model which already had learned to recognize lines, shapes and other features that increase in abstraction as we move toward the final layers of the model. We only had to retrain the last layers where we supplied training images of houses with and without pools.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Want to give it a try?\n",
    "\n",
    "We added some test images that you can use to test the model or you can download your own."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test images with pools:\n",
    "<img src=\"../data/test_images/house_with_pool/home-2008825__340.jpg\">\n",
    "<img src=\"../data/test_images/house_with_pool/villa-2366288__340.jpg\">\n",
    "### Test images without pools:\n",
    "<img src=\"../data/test_images/house_without_pool/holiday-house-177401__340.jpg\">\n",
    "<img src=\"../data/test_images/house_without_pool/weathered-2139859__340.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Before and after"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the inference engine with the original graph file and then with the retrained graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing with graph=output_graph_orig.pb\n",
      "\n",
      "../data/test_images/house_with_pool/villa-2366288__340.jpg\n",
      "house without pool (score = 0.50880)\n",
      "house with pool (score = 0.49120)\n",
      "../data/test_images/house_with_pool/home-2008825__340.jpg\n",
      "house without pool (score = 0.50004)\n",
      "house with pool (score = 0.49996)\n",
      "../data/test_images/house_without_pool/weathered-2139859__340.jpg\n",
      "house without pool (score = 0.50215)\n",
      "house with pool (score = 0.49785)\n",
      "../data/test_images/house_without_pool/holiday-house-177401__340.jpg\n",
      "house without pool (score = 0.50182)\n",
      "house with pool (score = 0.49818)\n",
      "\n",
      "Testing with graph=output_graph.pb\n",
      "\n",
      "../data/test_images/house_with_pool/villa-2366288__340.jpg\n",
      "house with pool (score = 0.99583)\n",
      "house without pool (score = 0.00417)\n",
      "../data/test_images/house_with_pool/home-2008825__340.jpg\n",
      "house with pool (score = 0.88350)\n",
      "house without pool (score = 0.11650)\n",
      "../data/test_images/house_without_pool/weathered-2139859__340.jpg\n",
      "house without pool (score = 0.92702)\n",
      "house with pool (score = 0.07298)\n",
      "../data/test_images/house_without_pool/holiday-house-177401__340.jpg\n",
      "house without pool (score = 0.69328)\n",
      "house with pool (score = 0.30672)\n"
     ]
    }
   ],
   "source": [
    "# Test with the test_images subdirs\n",
    "for graph in (output_graph_orig, output_graph):\n",
    "    print(\"\\nTesting with graph=%s\\n\" % graph)\n",
    "    for subdir in ('house_with_pool', 'house_without_pool'):\n",
    "        test_dir = os.path.join(test_images_dir, subdir)\n",
    "        for f in os.listdir(test_dir):\n",
    "            if f.endswith(JPEG_EXTENSIONS):\n",
    "                tf.reset_default_graph()\n",
    "                image = os.path.join(test_dir, f)\n",
    "                print(image)\n",
    "                %run ../image_retraining/label_image.py --image=$image --graph=$graph --labels=$output_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "The original results are not much better than a coin flip. This is the expected result as the Inception V3 model has not been trained for houses with or without pools.\n",
    "\n",
    "The new graph classifies the images correctly and with significant confidence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "I hope that you are now able to apply pre-trained models to your problem statements. Be sure that the pre-trained model you have selected has been trained on a similar dataset as the one that you wish to use it on. There are various architectures people have tried on different types of datasets and I strongly encourage you to go through these architectures and apply them to your own problem statements.\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
